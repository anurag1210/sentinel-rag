Sentinel-RAG

Production-Style Retrieval-Augmented Generation System

ğŸš€ Overview

Sentinel-RAG is a production-oriented Retrieval-Augmented Generation (RAG) system built using:
	â€¢	AWS Bedrock (Claude)
	â€¢	OpenSearch (vector database)
	â€¢	FastAPI (backend API)
	â€¢	Next.js (frontend UI)
	â€¢	Server-Sent Events (streaming inference)

The system enables document-grounded LLM responses with real-time token streaming and citation tracing.

â¸»

ğŸ§  Problem It Solves

Large Language Models hallucinate when answering without grounding.

Sentinel-RAG solves this by:
	â€¢	Embedding user queries
	â€¢	Performing semantic similarity search
	â€¢	Retrieving relevant document chunks
	â€¢	Injecting context into the LLM prompt
	â€¢	Streaming grounded responses back to the user

â¸»

ğŸ— Architecture


Flow:
	1.	User sends question via Next.js UI
	2.	API Route proxies to FastAPI backend
	3.	Query embedding generated (Bedrock Titan)
	4.	Vector similarity search in OpenSearch
	5.	Top-K chunks injected into Claude prompt
	6.	Claude response streamed via SSE
	7.	Frontend renders tokens in real time

â¸»

âš™ï¸ Key Engineering Features
	â€¢	Embedding-based semantic retrieval (1024-dim vectors)
	â€¢	Vector search using OpenSearch Serverless
	â€¢	RESTful API design (FastAPI)
	â€¢	Server-Sent Events (SSE) streaming
	â€¢	Prompt grounding to reduce hallucination
	â€¢	Metadata tracing (page number + chunk_id)
	â€¢	Latency tracking and cost-awareness considerations
	â€¢	Clean separation of backend and BFF layer

â¸»

ğŸ“Š Evaluation Considerations
	â€¢	Retrieval precision depends on chunk size & embedding quality
	â€¢	Latency primarily driven by LLM inference time
	â€¢	Streaming improves perceived latency
	â€¢	Grounding reduces hallucination risk

â¸»

ğŸ” Safety & Guardrails
	â€¢	Context-only answering enforced in prompt
	â€¢	Controlled output formatting
	â€¢	Designed to reject answers outside provided document context

â¸»

ğŸ›  Tech Stack

Backend:
	â€¢	Python
	â€¢	FastAPI (async)
	â€¢	boto3 (Bedrock integration)
	â€¢	OpenSearch
	â€¢	AWS4Auth

Frontend:
	â€¢	Next.js
	â€¢	TypeScript
	â€¢	Streaming via ReadableStream

Cloud:
	â€¢	AWS Bedrock
	â€¢	OpenSearch Serverless

â¸»

ğŸ“Œ Lessons Learned
	â€¢	LLM inference dominates latency
	â€¢	Streaming improves UX but not total compute time
	â€¢	Vector dimensional consistency is critical
	â€¢	Proper environment configuration avoids subtle failures
	â€¢	Prompt structure directly impacts hallucination behavior
